{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cbc241b-6a1e-43ce-9c65-cdd9b3142af2",
   "metadata": {},
   "source": [
    "### Q1 Implement convolution operation for a sample image of shape (H=6, W=6, C=1) with a random kernel of size (3,3) using torch.nn.functional.conv2d.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6aa155-8a16-40d2-a7ff-c17473269ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image-->torch.Size([1, 1, 6, 6])\n",
      "tensor([[[[0.6251, 0.6505, 0.4378, 0.1474, 0.5994, 0.9285],\n",
      "          [0.2749, 0.9148, 0.6964, 0.1909, 0.1309, 0.0587],\n",
      "          [0.0864, 0.8156, 0.4789, 0.6431, 0.9574, 0.9215],\n",
      "          [0.7104, 0.8992, 0.9068, 0.9772, 0.7405, 0.0013],\n",
      "          [0.8443, 0.4295, 0.3696, 0.4196, 0.2801, 0.0865],\n",
      "          [0.7689, 0.7415, 0.3323, 0.7337, 0.3579, 0.6738]]]])\n",
      "kernel-->torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n",
      "Output image for stride 1\n",
      "tensor([[[[2.4652, 3.5994, 3.0377, 2.2027, 2.0557, 1.7174],\n",
      "          [3.3673, 4.9803, 4.9753, 4.2821, 4.5778, 3.5964],\n",
      "          [3.7013, 5.7834, 6.5228, 5.7221, 4.6215, 2.8103],\n",
      "          [3.7854, 5.5407, 5.9394, 5.7732, 5.0273, 2.9874],\n",
      "          [4.3937, 6.0024, 5.8093, 5.1177, 4.2705, 2.1401],\n",
      "          [2.7841, 3.4860, 3.0261, 2.4931, 2.5515, 1.3983]]]])\n",
      "Stride=1 Padding=1 shape=torch.Size([1, 1, 6, 6])\n",
      "Stride=2 Padding=2 shape=torch.Size([1, 1, 4, 4])\n",
      "Stride=3 Padding=3 shape=torch.Size([1, 1, 4, 4])\n",
      "Stride=4 Padding=4 shape=torch.Size([1, 1, 3, 3])\n",
      "Stride=5 Padding=5 shape=torch.Size([1, 1, 3, 3])\n",
      "Stride=6 Padding=6 shape=torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "#module contains a collection of functions that operate on tensors but are not part of the neural network layers\n",
    "import torch.nn.functional as F \n",
    "#create an image of dimension hxw with 1 channel\n",
    "#as conv2d expects 4 dimensional tensor as input, we add extra dimension of 1 at index 0\n",
    "h,w=6,6\n",
    "image=torch.rand(1,1,h,w) \n",
    "print(f\"input image-->{image.shape}\\n{image}\")\n",
    "ksize=3\n",
    "kernel=torch.ones(1,1,ksize,ksize)\n",
    "print(f\"kernel-->{kernel.shape}\\n{kernel}\")\n",
    "strides=[1,2,3,4,5,6]\n",
    "pads=[1,2,3,4,5,6]\n",
    "for s,p in zip(strides,pads):\n",
    "    op=F.conv2d(image,kernel,stride=s,padding=p)\n",
    "    if s==1:\n",
    "        print(f\"Output image for stride 1\\n{op}\")\n",
    "    print(f\"Stride={s} Padding={p} shape={op.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88a8c9-42ab-4c5e-a760-9ebcb12639d0",
   "metadata": {},
   "source": [
    "### Q2 Apply torch.nn.Conv2d to the input image of Qn 1 with out-channel=3 and observe the output. Implement the equivalent of torch.nn.Conv2d using the torch.nn.functional.conv2D to get the same output. You may ignore bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5a6a41-a23a-4a07-b63b-a55cb1f4f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input image-->torch.Size([1, 1, 6, 6])\n",
      "tensor([[[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999],\n",
      "          [0.3971, 0.7544, 0.5695, 0.4388, 0.6387, 0.5247],\n",
      "          [0.6826, 0.3051, 0.4635, 0.4550, 0.5725, 0.4980],\n",
      "          [0.9371, 0.6556, 0.3138, 0.1980, 0.4162, 0.2843],\n",
      "          [0.3398, 0.5239, 0.7981, 0.7718, 0.0112, 0.8100],\n",
      "          [0.6397, 0.9743, 0.8300, 0.0444, 0.0246, 0.2588]]]])\n",
      "\n",
      "Output image using torch-->torch.Size([1, 3, 4, 4])\n",
      "tensor([[[[ 0.7547,  0.4656,  0.3594,  0.7186],\n",
      "          [ 0.5617,  0.7179,  0.5315,  0.5112],\n",
      "          [ 0.5165,  0.3959,  0.6457,  0.5841],\n",
      "          [ 0.6025,  0.8084,  0.7664,  0.1067]],\n",
      "\n",
      "         [[-0.4061, -0.3738, -0.5582, -0.2306],\n",
      "          [-0.4776, -0.3709, -0.3968, -0.3768],\n",
      "          [-0.3171, -0.3115, -0.2908, -0.4730],\n",
      "          [-0.4118, -0.4459, -0.3618, -0.5565]],\n",
      "\n",
      "         [[ 0.4015,  0.2539,  0.4177,  0.3974],\n",
      "          [ 0.2312,  0.3809,  0.3494,  0.4248],\n",
      "          [ 0.4048,  0.3176,  0.2225,  0.3113],\n",
      "          [ 0.7809,  0.3476, -0.0883,  0.0801]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "\n",
      "Output image using functional module-->torch.Size([1, 3, 4, 4])\n",
      "tensor([[[[ 0.7547,  0.4656,  0.3594,  0.7186],\n",
      "          [ 0.5617,  0.7179,  0.5315,  0.5112],\n",
      "          [ 0.5165,  0.3959,  0.6457,  0.5841],\n",
      "          [ 0.6025,  0.8084,  0.7664,  0.1067]],\n",
      "\n",
      "         [[-0.4061, -0.3738, -0.5582, -0.2306],\n",
      "          [-0.4776, -0.3709, -0.3968, -0.3768],\n",
      "          [-0.3171, -0.3115, -0.2908, -0.4730],\n",
      "          [-0.4118, -0.4459, -0.3618, -0.5565]],\n",
      "\n",
      "         [[ 0.4015,  0.2539,  0.4177,  0.3974],\n",
      "          [ 0.2312,  0.3809,  0.3494,  0.4248],\n",
      "          [ 0.4048,  0.3176,  0.2225,  0.3113],\n",
      "          [ 0.7809,  0.3476, -0.0883,  0.0801]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "\n",
    "h,w=6,6\n",
    "ksize=3\n",
    "torch.manual_seed(1)\n",
    "image=torch.rand(1,1,h,w)\n",
    "\n",
    "print(f\"input image-->{image.shape}\\n{image}\")\n",
    "\n",
    "conv_layer=torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=ksize, stride=1, padding=0)\n",
    "t_op=conv_layer(image)\n",
    "krnl=conv_layer.weight\n",
    "biases=conv_layer.bias\n",
    "\n",
    "Func_op=F.conv2d(image,krnl,bias=biases,stride=1,padding=0)\n",
    "\n",
    "print(f\"\\nOutput image using torch-->{t_op.shape}\\n{t_op}\")\n",
    "print(f\"\\nOutput image using functional module-->{Func_op.shape}\\n{Func_op}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9778d04-3f67-492f-ae72-d783be8b885a",
   "metadata": {},
   "source": [
    "### Q3 Implement CNN for classifying digits in MNIST dataset using PyTorch. Display the classification accuracy in the form of a Confusion matrix. Verify the number of learnable parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b178272b-fae8-4e52-b93a-fec7845feacb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69ff2687-8c30-40d7-ad18-48b0bd698d93",
   "metadata": {},
   "source": [
    "### Q4. Modify CNN of Qn. 3 to reduce the number of parameters in the network. Draw a plot of percentage drop in parameters vs accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c21abe-5abb-4a8a-92b1-c061a095a88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
