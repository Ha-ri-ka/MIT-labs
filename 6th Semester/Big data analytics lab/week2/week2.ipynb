{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### BELOW CODE IS TO WORK WITH SPARK IN COLAB, IGNORE IF NOT USING COLAB"
      ],
      "metadata": {
        "id": "BGBGNOnDOG29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-wmSzc9NzB-",
        "outputId": "0594edc4-6919-4cdd-f411-4add19c5c16a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,081 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,691 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,174 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,974 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,357 kB]\n",
            "Fetched 8,510 kB in 4s (2,181 kB/s)\n",
            "Reading package lists... Done\n",
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import os\n",
        "import sys\n",
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "os.environ['PYSPARK_PYTHON']=sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON']=sys.executable"
      ],
      "metadata": {
        "id": "-DEEOvMNN3lE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('week2').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "AXdiP2cBN8TH",
        "outputId": "8a201946-f590-43d5-d90f-a58b0fdd109a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d774810dea0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://44ccb3da03ae:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>week2</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe=spark.createDataFrame([\n",
        "    (1,'harika'),\n",
        "    (2,'Mano'),\n",
        "    (3,'Bhavya'),\n",
        "    (4,'Vaibhav'),\n",
        "    (5,'Aditi'),\n",
        "    (6,'Aarushi'),\n",
        "    (7,'Harry'),\n",
        "    (8,'Karthik'),\n",
        "    (9,'Aman'),\n",
        "    (10,'sidpai'),\n",
        "    (11,'Abhiram')\n",
        "],['id','name'])"
      ],
      "metadata": {
        "id": "Jr-aFpBWPZKg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdFjLy69PjC_",
        "outputId": "b14737fc-7680-40c7-f708-ada69a919844"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1| harika|\n",
            "|  2|   Mano|\n",
            "|  3| Bhavya|\n",
            "|  4|Vaibhav|\n",
            "|  5|  Aditi|\n",
            "|  6|Aarushi|\n",
            "|  7|  Harry|\n",
            "|  8|Karthik|\n",
            "|  9|   Aman|\n",
            "| 10| sidpai|\n",
            "| 11|Abhiram|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Implement a PySpark script that applies transformations like filter and withColumn on a\n",
        "DataFrame."
      ],
      "metadata": {
        "id": "q5gmQBJlPEC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "namesWithA=dataframe.filter(dataframe[\"name\"].startswith('A'))"
      ],
      "metadata": {
        "id": "DTOI8vkLPEqt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "namesWithA.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MjVfmaWQods",
        "outputId": "d3c156d6-973d-4f0b-a1d0-25c7cc1ad76c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  5|  Aditi|\n",
            "|  6|Aarushi|\n",
            "|  9|   Aman|\n",
            "| 11|Abhiram|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# incrID=df.withColumn(\"new column name\",df['existing column name'] operation)\n",
        "incrID=dataframe.withColumn(\"incremented\",dataframe['id']+2)\n",
        "incrID.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZnZ9F_xQvay",
        "outputId": "67f13e75-061a-4261-fd46-53249ff1fca8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----------+\n",
            "| id|   name|incremented|\n",
            "+---+-------+-----------+\n",
            "|  1| harika|          3|\n",
            "|  2|   Mano|          4|\n",
            "|  3| Bhavya|          5|\n",
            "|  4|Vaibhav|          6|\n",
            "|  5|  Aditi|          7|\n",
            "|  6|Aarushi|          8|\n",
            "|  7|  Harry|          9|\n",
            "|  8|Karthik|         10|\n",
            "|  9|   Aman|         11|\n",
            "| 10| sidpai|         12|\n",
            "| 11|Abhiram|         13|\n",
            "+---+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Write a PySpark script that performs actions like count and show on a DataFrame."
      ],
      "metadata": {
        "id": "O2RCSctfRWqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"total rows:{dataframe.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmijPEfARRH_",
        "outputId": "0db48091-40eb-47ec-c69d-a4e52414d38c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total rows:11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"total name with A:{dataframe.filter(dataframe['name'].startswith('A')).count()}\")\n",
        "print(f\"total name with b:{dataframe.filter(dataframe['name'].startswith('B')).count()}\")\n",
        "print(f\"total name with h:{dataframe.filter(dataframe['name'].startswith('h')).count()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q08JoHR7RZtg",
        "outputId": "f922aee7-9b42-45d6-d808-ff0f9efdcb1c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total name with A:4\n",
            "total name with b:1\n",
            "total name with h:1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "incrID.filter(incrID['incremented']>6).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jev1pay-SO5t",
        "outputId": "a4a29373-3053-4df2-b3a1-321480cf55e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----------+\n",
            "| id|   name|incremented|\n",
            "+---+-------+-----------+\n",
            "|  5|  Aditi|          7|\n",
            "|  6|Aarushi|          8|\n",
            "|  7|  Harry|          9|\n",
            "|  8|Karthik|         10|\n",
            "|  9|   Aman|         11|\n",
            "| 10| sidpai|         12|\n",
            "| 11|Abhiram|         13|\n",
            "+---+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Demonstrate how to perform basic aggregations (e.g., sum, average) on a PySpark\n",
        "DataFrame.\n"
      ],
      "metadata": {
        "id": "GYkCtqmQTMS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column, datatype in dataframe.dtypes:\n",
        "    print(f\"{column}: {datatype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDFebtybTu0o",
        "outputId": "3a92344c-22f6-4071-82ac-cd39f410b5fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id: bigint\n",
            "name: string\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = dataframe.withColumn(\"id\", dataframe[\"id\"].cast(\"int\"))"
      ],
      "metadata": {
        "id": "pAfx3E5gTMzk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column, datatype in dataframe.dtypes:\n",
        "    print(f\"{column}: {datatype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIOl8JN_UpOt",
        "outputId": "849ffac2-01cc-4aa7-fe33-369e59f9c973"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id: int\n",
            "name: string\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f\n",
        "dataframe.agg(f.avg(dataframe['id'])).show()\n",
        "dataframe.agg(f.sum(dataframe['id'])).show()\n",
        "dataframe.agg(f.max(dataframe['id'])).show()\n",
        "dataframe.agg(f.min(dataframe['id'])).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVziUuQuURD7",
        "outputId": "46e5754c-c03e-411f-8e2c-7956f4a0589b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|avg(id)|\n",
            "+-------+\n",
            "|    6.0|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|sum(id)|\n",
            "+-------+\n",
            "|     66|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|max(id)|\n",
            "+-------+\n",
            "|     11|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|min(id)|\n",
            "+-------+\n",
            "|      1|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Show how to write a PySpark DataFrame to a CSV file."
      ],
      "metadata": {
        "id": "INCk9lQuVmt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/writtenData.csv'\n",
        "dataframe.write.csv(path,mode='append')"
      ],
      "metadata": {
        "id": "Gfgw8JfFVnPW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Implement wordcount program in PySpark.\n"
      ],
      "metadata": {
        "id": "TnRV3vh-eQ25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc=SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
      ],
      "metadata": {
        "id": "YfEjYS_hg0h5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/sentences.txt'\n",
        "lines=sc.textFile(path)"
      ],
      "metadata": {
        "id": "sxpgeNbQeJXY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split the lines into words and create one big list"
      ],
      "metadata": {
        "id": "kLP-_XxRipLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=lines.flatMap(lambda line: line.split(\" \"))\n",
        "words.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2ZwXMe9gylX",
        "outputId": "235a84c7-cd94-410a-c139-17b95429ebd4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " 'gello',\n",
              " 'kello',\n",
              " 'wello',\n",
              " 'hello',\n",
              " 'hello',\n",
              " 'gello',\n",
              " 'kello',\n",
              " 'mano',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairRDD=words.map(lambda word: (word,1))\n",
        "pairRDD.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1C4tFxhiduS",
        "outputId": "85522349-0e29-4a4a-9af7-5eb361bd180c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hello', 1),\n",
              " ('gello', 1),\n",
              " ('kello', 1),\n",
              " ('wello', 1),\n",
              " ('hello', 1),\n",
              " ('hello', 1),\n",
              " ('gello', 1),\n",
              " ('kello', 1),\n",
              " ('mano', 1),\n",
              " ('', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "count the occurences of each unique word"
      ],
      "metadata": {
        "id": "w-QW-6z8jJan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts=pairRDD.reduceByKey(lambda x,y:x+y)\n",
        "sorted_counts=counts.sortBy(lambda x:x[1])\n",
        "sorted_counts.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZs_jpxUjHL1",
        "outputId": "74ec0b0b-91e3-43e4-f422-5bb081e3c720"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', 1), ('wello', 1), ('mano', 1), ('gello', 2), ('kello', 2), ('hello', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}