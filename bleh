import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
batch_size = 4
learning_rate = 0.001
epochs = 3
class CNNClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Conv2d(1,16, kernel_size=3,stride=2,padding=2),
                                 nn.ReLU(),
                                 nn.MaxPool2d((2,2), stride=2),
                                 nn.Conv2d(16,32, kernel_size=3,stride=1,padding=0),
                                 nn.ReLU(),
                                 nn.MaxPool2d((2,2), stride=2),
                                 nn.Conv2d(32,4, kernel_size=3,stride=1,padding=0),
                                 nn.ReLU(),
                                 nn.MaxPool2d((2,2), stride=2))
        self.classification_head = nn.Sequential(nn.Linear(4,20, bias=True),
                                                 nn.ReLU(),
                                                 nn.Linear(20,2, bias=True))
    def forward(self, x):
        features = self.net(x)
        return self.classification_head(features.view(batch_size, -1))


def GenerateGaussian(tindx):
    return torch.normal(tindx[0], tindx[1], (1, 42, 42))

class MyDataset(Dataset):
    def __init__(self, n):
        classes = {0: (0.5, 2), 1: (1, 2.5)}
        self.y = [torch.round(torch.rand(1))[0].long() for _ in range(n)]
        self.x = [GenerateGaussian(classes[self.y[i].item()]) for i in range(n)]

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

    def __len__(self):
        return len(self.x)


dataset = MyDataset(n=1200)
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_set, val_set = torch.utils.data.random_split(dataset, [1000,200])
train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)
model = CNNClassifier()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(epochs):
    for batch_idx, (data, target) in enumerate(train_loader):
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % 10000 == 0:
            print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')

model.eval()
correct = 0
total = 0
all_preds = []
all_labels = []

with torch.no_grad():
    for data, labels in test_loader:
        outputs = model(data)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        all_preds.extend(predicted.tolist())
        all_labels.extend(labels.tolist())

accuracy = correct / total
print(f'\nAccuracy on the test set: {accuracy:.2%}')

cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(10, 8))
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f'\nNumber of learnable parameters in the model: {num_params}')
